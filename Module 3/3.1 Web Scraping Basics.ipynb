{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250edc4c448fed01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Web Scraping Basics\n",
    "\n",
    "Web scraping is a powerful tool to have as part of a data science, analyst, or engineering toolkit. \n",
    "It allows you to extract data from websites and use it for your own projects or analysis.\n",
    "\n",
    "In football analytics, web scraping can be used to collect data on players, teams, and matches.\n",
    "Most of the data that teams are using is coming from large and expensive data providers, but we can collect some of this data via web scraping.\n",
    "\n",
    "In this notebook, we will cover the basics of web scraping using Python and the `requests` and `BeautifulSoup` libraries.\n",
    "\n",
    "#### Web Scraping Steps\n",
    "1. Send an HTTP request to the URL of the webpage you want to access\n",
    "2. Get the HTML content of the webpage\n",
    "3. Parse the HTML content\n",
    "4. Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e872a85305f60b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:39:28.531377Z",
     "start_time": "2024-03-09T22:39:28.409599Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca0972c61b02590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:39:29.629878Z",
     "start_time": "2024-03-09T22:39:28.866764Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll start by scraping a normal ecommerce website, Gymshark.com\n",
    "# First, we'll send an HTTP request to the URL of the webpage we want to access\n",
    "\n",
    "url = \",\"\n",
    "\n",
    "# Let's also go get our headers to pass in\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = requests.get(\n",
    "    url,\n",
    "    headers=headers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb0b8d512a7053e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:39:33.644964Z",
     "start_time": "2024-03-09T22:39:33.638424Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the status code of the response to see if the request was successful\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896f750495c5d66",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Status Codes\n",
    "You'll mainly see the following status codes when web scraping:\n",
    "\n",
    "- 200, the request was successful\n",
    "- 404, the page was not found\n",
    "- 403, access to the page was forbidden which means we need to add headers to our request or use a proxy\n",
    "- 500, there was an internal server error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c665a9b6ec39b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:40:21.485069Z",
     "start_time": "2024-03-09T22:40:21.463687Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can then parse the HTML content of the webpage using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613c9912bb9b7fc5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:49:47.449420Z",
     "start_time": "2024-03-09T22:49:47.442516Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's use some css selectors to extract the data we want\n",
    "# Let's start off by getting the title of the product\n",
    "# To select just one element, we can use the `select_one` method\n",
    "title = soup.select_one('h1[class=\"product-information_title__3jR8K\"]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d6763c6bc4096b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:50:22.281365Z",
     "start_time": "2024-03-09T22:50:22.239073Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now lets get the price of the product\n",
    "price = soup.select_one('div[class=\"product-information_price__pEWjj\"]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "559e7b8b71f55096",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:54:47.752275Z",
     "start_time": "2024-03-09T22:54:47.749146Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now let's try getting multiple elements\n",
    "# To do this, we can use the `select` method\n",
    "\n",
    "# Let's get the colors of the product\n",
    "# We can do pathings with css selectors to get to the element we want\n",
    "colors = soup.select('div[class=\"variants_variants__C9MOx\"] a img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd3a0df43986506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:54:48.288325Z",
     "start_time": "2024-03-09T22:54:48.286012Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black', 'Light Pink', 'Navy', 'Core Olive', 'Stone Grey', 'Light Grey Marl', 'Natural Sage Green', 'Faded Blue']\n"
     ]
    }
   ],
   "source": [
    "# That returned a list of elements, so we can loop through them to get the text\n",
    "colors = [x.attrs['alt'].replace(title + ' in', '').strip() for x in colors]\n",
    "print(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5bd38dfe6ab92",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Selector helpers\n",
    "- `soup.select_one` returns the first element that matches the selector\n",
    "- `soup.select` returns a list of elements that match the selector\n",
    "\n",
    "We can also use different ways to select elements so that it can be a wildcard, for example:\n",
    "- `*=` -> class name contains a value\n",
    "- `:-soup-contains()` -> text contains a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7288da1d86a0531",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T23:06:01.680879Z",
     "start_time": "2024-03-09T23:06:01.676384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# So we could rewrite colors as \n",
    "colors = soup.select(f'img[alt*=\"{title} in \"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "772ab5d79b0601da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T23:08:37.921883Z",
     "start_time": "2024-03-09T23:08:37.918186Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# if we wanted to get the button that had the \"add to bag\"\n",
    "add_to_bag = soup.select_one('button:-soup-contains(\"Add to bag\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e6cb4154054df",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Exercise:\n",
    "\n",
    "Now that you've seen how to get the colors, use the same method to get the sizes of the product\n",
    "Make sure that you only select the sizes for this specific product and not all the sizes on the page (There should be 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea2f999f000f26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T22:59:06.331763Z",
     "start_time": "2024-03-09T22:59:06.322420Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sizes = \"YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f839278b1fee9ac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-09T23:09:46.914323Z",
     "start_time": "2024-03-09T23:09:46.911345Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This is the answer\n",
    "sizes = soup.select('button[data-locator-id*=\"pdp-size\"]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
